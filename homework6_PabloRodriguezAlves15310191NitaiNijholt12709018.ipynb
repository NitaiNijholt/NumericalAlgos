{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb459a25-a4dd-415c-ba1f-ad86a4367214",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-06d3e5bf55c941ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Homework set 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8d273-e303-413d-9abc-3fc0ccfa3595",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-736ff6bc3e0d0696",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected (in the menubar, select Kernel â†’ Restart Kernel and Run All Cells...).\n",
    "\n",
    "Please **submit this Jupyter notebook through Canvas** no later than **Mon Dec. 11, 9:00**. **Submit the notebook file with your answers (as .ipynb file) and a pdf printout. The pdf version can be used by the teachers to provide feedback. A pdf version can be made using the save and export option in the Jupyter Lab file menu.**\n",
    "\n",
    "Homework is in **groups of two**, and you are expected to hand in original work. Work that is copied from another group will not be accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923083fc-e388-4eb6-8b57-3f256593d94e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b13bc5ed16bce8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 0\n",
    "Write down the names + student ID of the people in your group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785f9c4-829c-4226-b999-948ebf5864a9",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fd464f55ba436b1c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Pablo Rodriguez Alves - 15310191\n",
    "Nitai Nijholt - 12709018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf801bfc-3576-4853-8c5f-3f1f5d2a88c4",
   "metadata": {},
   "source": [
    "# About imports\n",
    "Please import the needed packages by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5293e8-b5f4-42fa-a94d-b764aec592da",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "N.B.1 tentative points for each part are: 2+1.5+2+2+1.5 (and one point for free gives 10).\n",
    "\n",
    "N.B.2 you are to implement the methods yourself.\n",
    "\n",
    "Given a function $f$, let $T(f,a,b,m)$ denote the composite trapezoid rule with $m$ subintervals over the interval $[a,b]$. \n",
    "## (a)\n",
    "Approximate the integral of $x^{-3}$ over $[a,b] = [ \\frac{1}{10}, 100 ]$ by the composite trapezoid rule $T(f,a,b,m)$ for $m = 2^k$. Find the smallest $k$ such that the exact error is less than $\\epsilon = 10^{-3}$. Explain the slow convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f(x) = x^{-3}$\n",
    "\n",
    "Then the integral of $f(x)$ is $F(X) = \\frac{-1}{2x^{2}}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our function of interest and its integral\n",
    "def func(x):\n",
    "    return x**-3\n",
    "\n",
    "def F(x):\n",
    "    return -1/(2*x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T(func,a,b,m):\n",
    "    \"Heath p. 354\"\n",
    "\n",
    "    # Compute number of points: one more than subintervals\n",
    "    p = m+1\n",
    "\n",
    "    # Calculate the width of each trapezoid\n",
    "    h = (b-a)/(p)\n",
    "    \n",
    "    # Evaluate the function at the first point (a) and multiply by 0.5\n",
    "    first_eval = 0.5 * func(a)\n",
    "    \n",
    "    # Evaluate the function at the last point (b) and multiply by 0.5\n",
    "    last_eval = 0.5 * func(b)\n",
    "    \n",
    "    # Initialize cumulative sum of function evaluations with first and last evaluations\n",
    "    cummulative_evals = first_eval + last_eval\n",
    "\n",
    "    # Loop over each subinterval excluding the endpoints\n",
    "    for j in range(1, p-2):\n",
    "        # Calculate the x-coordinate for the j-th trapezoid's top edge\n",
    "        x_j = a + j * h\n",
    "        \n",
    "        # Add the function value at x_j to the cumulative sum\n",
    "        cummulative_evals += func(x_j)\n",
    "\n",
    "    # Multiply the cumulative sum by the width of the trapezoids to get the final integral approximation\n",
    "    return h * cummulative_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_value 49.99994999999999\n",
      "k 3 I_trap [5550.00943311] Error_trap [5500.00948311]\n",
      "k 4 I_trap [2938.26848819] Error_trap [2888.26853819]\n",
      "k 5 I_trap [1513.75647178] Error_trap [1463.75652178]\n",
      "k 6 I_trap [768.89100278] Error_trap [718.89105278]\n",
      "k 7 I_trap [388.6568728] Error_trap [338.6569228]\n",
      "k 8 I_trap [198.68541397] Error_trap [148.68546397]\n",
      "k 9 I_trap [108.04895499] Error_trap [58.04900499]\n",
      "k 10 I_trap [69.34160828] Error_trap [19.34165828]\n",
      "k 11 I_trap [55.5533005] Error_trap [5.5533505]\n",
      "k 12 I_trap [51.45849196] Error_trap [1.45854196]\n",
      "k 13 I_trap [50.36982817] Error_trap [0.36987817]\n",
      "k 14 I_trap [50.09277] Error_trap [0.09282]\n",
      "k 15 I_trap [50.02317791] Error_trap [0.02322791]\n",
      "k 16 I_trap [50.0057585] Error_trap [0.0058085]\n",
      "< 0.01\n",
      "Done!, k = 16\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "a = 1/10\n",
    "b = 100\n",
    "epsilon = 1/100\n",
    "\n",
    "# Get real value of the integral first\n",
    "real_value = F(b) - F(a)\n",
    "print('real_value', real_value)\n",
    "\n",
    "# Loop over values of k\n",
    "for k in range(3,30):\n",
    "\n",
    "    # Get number of subintervals for current k\n",
    "    m = 2**k\n",
    "\n",
    "    # Compute the approximation and its error\n",
    "    I_trap = np.array([T(func,a,b,m)])\n",
    "    err_trap = np.abs(real_value - I_trap)\n",
    "\n",
    "    # Print values to check evolution\n",
    "    print('k',k,'I_trap',I_trap,'Error_trap',err_trap)\n",
    "\n",
    "    # Check if our error is smaller than the tolerance\n",
    "    if err_trap < epsilon:\n",
    "        print('<',epsilon)\n",
    "        print('Done!, k =',k)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Convergence was obtained for $k=16$, which entails a total of $m=2^{16}=65536$ subintervals, which is a high number considering the considered interval has a length of barely $100$ and the tolerance used, $1/100$, was very high. This slow convergence ban be explained. Firstly, values making most of the integration are near $a$, the left extreme of the interval, with our function sharply decreasing for the entire interval, having $f(a)\\approx 1000$ and $f(b)=10^{-6}$. Because this method always divides the interval evenly, it is specially slow to converge for functions like this one, that have most of its important values in the extremes, for it has to create many subintervals before getting enough points in that region. In short, all points on the right side of the interval evaluate to approximately $0$ and thus have a minimal contribution to the result. Despite this, the algorithm always creates further divisions in that region, creating more useless evaluating points on the right side of the interval, and thus resulting specially inneficient for convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3608cd-207d-4679-8fd1-5ee7611d3929",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (b)\n",
    "\n",
    "To improve the convergence rate of the above problem, we may use an adaptive strategy, as discussed in the book and the lecture. Consider the following formulas for approximate integration\n",
    "$$\\begin{aligned}\n",
    "I_1(f,a,b) = {}& T(f,a,b,1) \\\\\n",
    "I_2(f,a,b) = {}& T(f,a,b,2) .\n",
    "\\end{aligned}$$\n",
    "Show, based on the error estimates for the trapezoid rule using the Taylor series (book example 8.2) that the error in $I_2$ can be estimated by a formula of the form \n",
    "$$E_2 = C (I_1 - I_2)$$\n",
    "and determine the constant $C$ (if you can't find $C$, you may take $C = 0.5$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "We want to prove the main equation $E_2 = C(I_1-I_2)$ and find $C$. \n",
    "\n",
    "For this, let's first evaluate $I_1$ and $I_2$:\n",
    "\n",
    "$I_1=\\frac{(b-a)}{2}\\cdot(f(a)+f(b))$\n",
    "\n",
    "$I_2=\\frac{(b-a)}{2}\\cdot(\\frac{f(a)}{2}+f(m)+\\frac{f(b)}{2})$\n",
    "\n",
    "We also define the error of $I_2$ as\n",
    "\n",
    "$E_2 = I_2 - I$\n",
    "\n",
    "In order to compute $E_2$, we approximate $I$ with the midpoint expansion of $[a,b]$ ($m$ now representing the midpoint at $\\frac{a+b}{2}$), disregaring the higher order terms (see Heath p.346):\n",
    "\n",
    "$I = I(f)=f(m)(b-a) + \\frac{f^{''}(m)}{24}(b-a)^3 + \\frac{f^{''''}(m)}{1920}(b-a)^5 + ... $\n",
    "\n",
    "\n",
    "\n",
    "$ \\therefore I \\thickapprox f(m)(b-a)$\n",
    "\n",
    "We now need to solve for $f(m)$, for which we consider the Taylor series of $f(x)$, disregaring the higher order terms again::\n",
    "\n",
    "$f(x)=f(m) + \\frac{f^{'}(m)}{1}(x-m) + \\frac{f^{''}(m)}{2}(x-m)^2 + \\frac{f^{'''}(m)}{6}(x-m)^3 ...$ (\n",
    "\n",
    "$ \\therefore f(x) \\thickapprox f(m)$\n",
    "\n",
    "Adding $f(a)$ and $f(b)$ and solving for $f(m)$\n",
    "\n",
    "$f(a)+f(b)=2f(m)$\n",
    "\n",
    "$ \\therefore f(m) = \\frac{1}{2}(f(a)+f(b))$\n",
    "\n",
    "\n",
    "Substituting $f(m)$ into $I(f)$\n",
    "\n",
    "$I = \\frac{1}{2}(f(a)+f(b))(b-a)$\n",
    "\n",
    "$ \\therefore I = I_1$\n",
    "\n",
    "Observing the left side of the main equation\n",
    "\n",
    "$ E_2 = I_2 - I = I_2 - I_1 = \\frac{(b-a)}{2}\\cdot(\\frac{f(a)}{2} + f(m) + \\frac{f(b)}{2} -f(a) -f(b)) $\n",
    "\n",
    "$ \\therefore E_2 = \\frac{(b-a)}{2}\\cdot(-\\frac{f(a)}{2} + f(m) - \\frac{f(b)}{2}  ) = \\frac{-(b-a)}{2}\\cdot(\\frac{f(a)}{2} - f(m) + \\frac{f(b)}{2}  )$\n",
    "\n",
    "$ \\therefore E_2 = -1(I_1-I_2)$\n",
    "\n",
    "Finally\n",
    "\n",
    "$E_2 = C(I_1-I_2) \\leftrightarrow -1(I_1 - I_2 ) = C(I_1-I_2) \\leftrightarrow$\n",
    "\n",
    "$ C = -1 $\n",
    "\n",
    "$\\square$\n",
    "\n",
    "Should we had defined $E_2$ as $E_2 = I - I_2$, it would have yielded $C=1$, which is consistent, for $E_2$ can be defined in two ways that differ in sign (as $I-I_2=-1(I_2-I)$), subsequently allowing $C$ to have either a positive or negative sign. Importantly, both error definitions are correct (the exercise did not specify either as preferable), for when computing the error, we must take its absolute value, with both constants producing the same result, and thus being equivalent algorithmically. For this reason, both proofs, while yielding a different $C$ constant, are equivalent and both $C$ values can be used interchangeably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979c60b-8ecd-45bd-8b88-acecfdd03940",
   "metadata": {},
   "source": [
    "## (c)\n",
    "An adaptive strategy for computing the integral on an interval $[a,b]$ now is: Compute $I_2$ and $E_2$, and accept $I_2$ as an approximation when the estimated error $E_2$ is less or equal than a desired tolerance $\\epsilon$.  Otherwise, apply the procedure to \n",
    "$\\int_a^{\\frac{b+a}{2}} f(x) \\, dx$ and $\\int_{\\frac{b+a}{2}}^b f(x) \\, dx$ with tolerances $\\frac{\\epsilon}{2}$.\n",
    "\n",
    "Write a recursive python routine that implements the adaptive strategy.\n",
    "\n",
    "Then apply this routine to the function $x^{-3}$ with $a, b, \\epsilon$ as before. What is the exact error in the obtained approximation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_integral(f,a,b,tolerance):\n",
    "    # Using the algorithm in (c)\n",
    "    I1 = ((b-a)/2) * (f(a) + f(b))\n",
    "    I2 = ((b-a)/2) * (f(a)/2 + f((a+b)/2) + f(b)/2)\n",
    "\n",
    "    # Using the formula from (b)\n",
    "    E2 = np.abs(-I1+I2)\n",
    "\n",
    "    if E2 < tolerance:\n",
    "        return I2\n",
    "    else:\n",
    "        return recursive_integral(f,a,(a+b)/2,tolerance/2) + recursive_integral(f,(a+b)/2,b,tolerance/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 50.000594757608695\n",
      "Exact error: 0.0006447576087040829\n"
     ]
    }
   ],
   "source": [
    "a = 1/10\n",
    "b = 100\n",
    "epsilon = 1/100\n",
    "\n",
    "value = recursive_integral(func,a,b,epsilon)\n",
    "exact_error = np.abs(real_value - value)\n",
    "\n",
    "print('Value:',value)\n",
    "print('Exact error:',exact_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38054a2a-24b4-4467-8dad-4ad193400213",
   "metadata": {},
   "source": [
    "## (d)\n",
    "Modify the code of (c) so that the number of function evaluations is counted and that no unnecessary function evaluations are performed. Compare the number of function evaluations used in the adaptive strategy of (c) with the result of (a). \n",
    "(*Hint*: To count the number of function evaluations, you may use a global variable that is incremented by the function each time it is called.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global variable\n",
    "FUNCTION_CALLS = 0\n",
    "\n",
    "# Update the function by letting it know that the variable \n",
    "# FUNCTION_CALLS refers to the global variable (to update it)\n",
    "def func(x):\n",
    "    global FUNCTION_CALLS\n",
    "    FUNCTION_CALLS += 1\n",
    "    return x**-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the integral function\n",
    "def recursive_integral_optimized(f,a,b,tolerance,f_a,f_b):\n",
    "    # Optimization 1: Set f(a) and f(b) as function parameters \n",
    "    f_a = f_a\n",
    "    f_b = f_b\n",
    "\n",
    "    # Optimization 2: Only compute f(m) for each call\n",
    "    f_m = f((a+b)/2)\n",
    "\n",
    "    # Using the algorithm (c)\n",
    "    I1 = ((b-a)/2) * (f_a + f_b)\n",
    "    I2 = ((b-a)/2) * (f_a/2 + f_m + f_b/2)\n",
    "\n",
    "    # Using the formula from (b)\n",
    "    E2 = -(I1-I2)\n",
    "\n",
    "    if np.abs(E2) < tolerance:\n",
    "        return I2\n",
    "    else:\n",
    "        # Send the f_m to next iterations\n",
    "        return recursive_integral_optimized(f,a,(a+b)/2,tolerance/2,f_a,f_m) + recursive_integral_optimized(f,(a+b)/2,b,tolerance/2,f_m,f_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original function calls: 26625\n",
      "Optimized function calls: 5327\n",
      "\n",
      "Fraction of calls required: 0.2\n",
      "Calls saved: 21298\n",
      "\n",
      "Original function time: 0.0171 [s]\n",
      "Optimized function time: 0.0112 [s]\n",
      "\n",
      "Fraction of time required: 0.654\n",
      "Time saved: 0.0059130999998160405 [s]\n",
      "\n",
      "Values still the same? True\n",
      "Errors still the same? True\n"
     ]
    }
   ],
   "source": [
    "a = 1/10\n",
    "b = 100\n",
    "epsilon = 1/100\n",
    "\n",
    "# Reset counter\n",
    "FUNCTION_CALLS = 0\n",
    "\n",
    "# First count calls to original function\n",
    "start_time = time.perf_counter()\n",
    "value = recursive_integral(func,a,b,epsilon)\n",
    "time_1 = time.perf_counter() - start_time\n",
    "\n",
    "exact_error = np.abs(real_value - value)\n",
    "calls = FUNCTION_CALLS\n",
    "\n",
    "\n",
    "# Reset counter\n",
    "FUNCTION_CALLS = 0\n",
    "\n",
    "# Call the new function\n",
    "start_time = time.perf_counter()\n",
    "value2 = recursive_integral_optimized(func,a,b,epsilon,func(a),func(b))\n",
    "time_2 = time.perf_counter() - start_time\n",
    "\n",
    "exact_error2 = np.abs(real_value - value2)\n",
    "calls2 = FUNCTION_CALLS\n",
    "\n",
    "print('Original function calls:', calls)\n",
    "print('Optimized function calls:', calls2)\n",
    "print()\n",
    "print('Fraction of calls required:',round(calls2/calls,3))\n",
    "print('Calls saved:',calls-calls2)\n",
    "print()\n",
    "print('Original function time:',round(time_1,4),'[s]')\n",
    "print('Optimized function time:',round(time_2,4),'[s]')\n",
    "print()\n",
    "print('Fraction of time required:',round(time_2/time_1,3))\n",
    "print('Time saved:',time_1-time_2, '[s]')\n",
    "print()\n",
    "print('Values still the same?',value == value2)\n",
    "print('Errors still the same?',exact_error == exact_error2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "With our optimizations, our function now also receives $f(a)$ and $f(b)$ as parameters and each call only has to compute $f(m)$. In this way, by storing calculations that have been done (which do not need to be computed again) we can save on function evaluations. These improvements now allow us to produce the same results of with one fifth of calls (5327 vs. 26625) and in around 40 to 65 \\% of the original time. These improvements are huge, specially considering that the code complexity has barely increased, thanks to recursiveness. In short, the adaptive strategy of (c) is preferable to the original result of (a), as this method finally allows to focus the subintervals in the left side of the $[a,b]$ interval which, as already shown, contains all important contributions to the integral result. In this way, the algorithm can adapt to our function, converging for its right side fast to then focus exclusively on the left side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3709eb-4f5e-493a-865f-31f31510ea5e",
   "metadata": {},
   "source": [
    "## (e)\n",
    "In the course of executing the recursive procedure, some subintervals are refined (split in two subintervals) while others aren't as a result of the choices made by the algorithm. It turns out that the choices made by this algorithm are not always optimal. Other algorithms, that decide in a different way which subinterval needs to be refined, may be more efficient in the sense that they require less function evaluations (while using the same formulas for the approximate integral and the approximate error associated with a subinterval).\n",
    "\n",
    "Can you explain why this is the case? Discuss briefly possible alternative approaches.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384b2a4-1738-440c-bc1a-9531dca3e126",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "At a high level, these alternative approaches test assumptions of the function we are integrating inside the integration algorithm and use that information to make smarter subinterval refinement decisions. Importantly, these additional steps, which are then introduced to reduce function evaluations, are expected to themselves cost computational resources, potentially increasing the final computation times and thus motivating a compromise between the complexity of the algorithm and its use cases. With this is mind is easy to see that our method is simple, as it only evaluates the original function $f(x)$ at certain points that are progresively subdivided with the same rate. For this reason, it cannot easily adapt to functions that may have certain subregions with a rapidly changing behavior (for which it would be better to refine them with more subintervals) but instead, risks doing unnecessary refinements in other regions, contributing to more function evaluations. For this reason, a possible approach would be to keep track of the rate of change, and take this rate of change into consideration when deciding which intervals to further refine. Also, as our current model only compares the error with $\\epsilon$ in order to decide whether to refine or not the interval, always refining it in the same way no matter how big the error is, another approach would be to keep track of the error estimates, refining the subintervals (and thus the number of calls) in accordance with the error size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
